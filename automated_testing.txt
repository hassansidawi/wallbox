In traditional software development, code can be built and tests can be automated in production-like environments.
This process is repeatable and predictable, promotes code quality, catch bugs early, reduce the risk of regressions and accelerate development iterations.
It is very mature in the web development space, and increasingly so in mobile app development as well.

Though things are increasingly complicated in the embedded world, the end user’s expectations are the same — modern systems should just work.
The interplay between hardware and software made things more difficult. A different approach when hardware is involved.
It isn’t as easy to isolate code because of the dependencies to the underlying hardware that can hardly be overlooked.

Habitually, systems involving integration between hardware and software require:
- The ability to gather sensor data by reacting to a variety of inputs like touch, voice and motion tracking;
- Different types of interoperating hardware (boards, machines, cameras);
- Cloud-based servers, mobile and web applications from which the devices can be monitored and controlled;
- A Device API to enable routine data and device diagnostics pulls to cloud servers, as well as functionality manipulation.

Furthermore, embedded systems are subjected to regulatory requirements such as IEC 61508 and MISRA to ensure the safety and reliability of programmable electronic devices.

Programming languages used in embedded systems tend to be either C or C++. These languages, more low-level than those used in web development. Lack of code portability means that cross-compilation is required between development and target environments.

In general, the ability to decouple programming logic from hardware makes for easier subsystem testing. 
To mitigate problems around test data quality, testers record input data from real world users and environmental data, to be replayed in test environments.
This helps to keep test environments as production-like as possible.
Compliance testing against regulatory requirements can be partially automated using static analysis tools.

Mostly, modern approaches to testing composite applications featuring embedded software involve some form of simulation.
Much like the way mobile app developers use emulators like Perfecto and Sauce Labs to recreate test conditions across a variety of smartphones, embedded software developers resort to simulation software to abstract away parts of their continuous testing environments.
In particular, simulations resolve the problems of hardware availability and accelerate tests against various versions, screen sizes, and other hardware properties.

These virtualized environments are specific to each application and tend to be expensive to set up, but as is the case in traditional continuous integration, the initial effort pays back for itself many times over as the project’s life extends.
Not only do they afford much more flexibility in test setups and scale, but they help put testing at the center of the team’s preoccupations.

In a virtual hardware lab, the Hardware and the outside world are replaced by a simulation so that the entire setup becomes purely software-based.

These simulators are becoming increasingly sophisticated, supporting performance and memory testing, security testing, and they even allow for edge case tests and fault injection, like dropped connections or electromagnetic interference creating noise in sensor data.

Nevertheless, a decent quality assurance process using real hardware is still necessary at the tail end of each major testing cycle. There are several reasons why teams may never rely entirely on simulations. For one thing, errors and approximations in the simulation can cause imperfections that must be caught before shipping. In addition, human interaction testing and emergent behaviors can’t be simulated, and a person’s emotional response to even a perfectly functional hardware-enabled system can be difficult to predict. A combination of white-box testing and black-box testing is usually employed during the QA phase to detect problems that might have fallen through the simulation’s cracks.
